{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQdblWeksdKEE1DVimRy07",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nivethithanm/math-for-ml/blob/main/Calculus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "msjDR3ii-u6w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent 1 Variable"
      ],
      "metadata": {
        "id": "obiQAGjq-hDZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function $f\\left(x\\right)=e^x - \\log(x)$ (defined for $x>0$) is a function of one variable which has only one **minimum point** (called **global minimum**). However, sometimes that minimum point cannot be found analytically - solving the equation $\\frac{df}{dx}=0$. It can be done using a gradient descent method.\n",
        "\n",
        "To implement gradient descent, you need to start from some initial point $x_0$. Aiming to find a point, where the derivative equals zero, you want to move \"down the hill\". Calculate the derivative $\\frac{df}{dx}(x_0)$ (called a **gradient**) and step to the next point using the expression:\n",
        "\n",
        "$$x_1 = x_0 - \\alpha \\frac{df}{dx}(x_0),\\tag{1}$$\n",
        "\n",
        "where $\\alpha>0$ is a parameter called a **learning rate**. Repeat the process iteratively. The number of iterations $n$ is usually also a parameter.\n",
        "\n",
        "Subtracting $\\frac{df}{dx}(x_0)$ you move \"down the hill\" against the increase of the function - toward the minimum point. So, $\\frac{df}{dx}(x_0)$ generally defines the direction of movement. Parameter $\\alpha$ serves as a scaling factor.\n",
        "\n",
        "Now it's time to implement the gradient descent method and experiment with the parameters!\n",
        "\n",
        "First, define function $f\\left(x\\right)=e^x - \\log(x)$ and its derivative $\\frac{df}{dx}\\left(x\\right)=e^x - \\frac{1}{x}$:"
      ],
      "metadata": {
        "id": "yiE_1b3x-oZi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i18HlEPP-ZnW"
      },
      "outputs": [],
      "source": [
        "def function(x):\n",
        "    res = np.exp(x) - np.log(x)\n",
        "    return res\n",
        "\n",
        "def dfdx(x):\n",
        "    res = np.exp(x) - 1/x\n",
        "    return res"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define gradient descent\n",
        "\n",
        "def gradient_descent(x0, lr, n_iterations):\n",
        "    x = x0\n",
        "    for i in range(n_iterations):\n",
        "        x_new = x - lr * dfdx(x)\n",
        "        x = x_new\n",
        "        print(f\"Iteration {i+1}: x = {x}, f(x) = {function(x)}, f'(x) = {dfdx(x)}\")"
      ],
      "metadata": {
        "id": "PSUbiaBp_WcO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of iterations\n",
        "n_iterations = 10\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Initial guess\n",
        "x0 = 0.1\n",
        "\n",
        "# Run gradient descent\n",
        "gradient_descent(x0, lr, n_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M6cqWuJ2-4Gb",
        "outputId": "4f644fa4-4cfa-4501-8767-0728f1e838c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: x = 0.9894829081924353, f(x) = 2.7004160040504135, f'(x) = 1.6792143401063886\n",
            "Iteration 2: x = 0.8215614741817965, f(x) = 2.4705964464952412, f'(x) = 1.0568535650144248\n",
            "Iteration 3: x = 0.715876117680354, f(x) = 2.3802265621133234, f'(x) = 0.6490886805687204\n",
            "Iteration 4: x = 0.650967249623482, f(x) = 2.3466904773486013, f'(x) = 0.3812189396082468\n",
            "Iteration 5: x = 0.6128453556626573, f(x) = 2.3353181873510693, f'(x) = 0.21394252274252046\n",
            "Iteration 6: x = 0.5914511033884052, f(x) = 2.331784353776426, f'(x) = 0.11585124694753945\n",
            "Iteration 7: x = 0.5798659786936513, f(x) = 2.330757352946413, f'(x) = 0.061262657820157385\n",
            "Iteration 8: x = 0.5737397129116356, f(x) = 2.3304716903269487, f'(x) = 0.03194160266668833\n",
            "Iteration 9: x = 0.5705455526449668, f(x) = 2.3303942633788566, f'(x) = 0.016523567075364953\n",
            "Iteration 10: x = 0.5688931959374303, f(x) = 2.3303735762847615, f'(x) = 0.008511817224204021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gradient Descent 2 Variable"
      ],
      "metadata": {
        "id": "wKKjFNHuNwIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function(x, y):\n",
        "    res = x**2 + y**2\n",
        "    return res\n",
        "\n",
        "def dfdx(x, y):\n",
        "    return 2*x\n",
        "\n",
        "def dfdy(x, y):\n",
        "    return 2*y\n",
        "\n",
        "# Define gradient descent\n",
        "\n",
        "def gradient_descent(x0, y0, lr, n_iterations):\n",
        "    x, y = x0, y0\n",
        "    for i in range(n_iterations):\n",
        "        x, y = x - lr * dfdx(x, y), y - lr * dfdy(x, y)\n",
        "        print(f\"Iteration {i+1}: (x, y) = {(float(x), float(y))}, f(x, y) = {float(function(x, y))}, grad(x, y) = {(float(dfdx(x, y)), float(dfdy(x, y)))}\")"
      ],
      "metadata": {
        "id": "cbm0vT_k_5cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of iterations\n",
        "n_iterations = 25\n",
        "\n",
        "# Learning rate\n",
        "lr = 0.1\n",
        "\n",
        "# Initial guess\n",
        "x0, y0 = 0.5, 0.5\n",
        "\n",
        "# Run gradient descent\n",
        "gradient_descent(x0, y0, lr, n_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGO6oLkSOB0j",
        "outputId": "2b5bdfe4-e32c-4af7-c5b0-8488ca81a586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: (x, y) = (0.4, 0.4), f(x, y) = 0.32000000000000006, grad(x, y) = (0.8, 0.8)\n",
            "Iteration 2: (x, y) = (0.32, 0.32), f(x, y) = 0.2048, grad(x, y) = (0.64, 0.64)\n",
            "Iteration 3: (x, y) = (0.256, 0.256), f(x, y) = 0.131072, grad(x, y) = (0.512, 0.512)\n",
            "Iteration 4: (x, y) = (0.2048, 0.2048), f(x, y) = 0.08388608, grad(x, y) = (0.4096, 0.4096)\n",
            "Iteration 5: (x, y) = (0.16384, 0.16384), f(x, y) = 0.05368709120000001, grad(x, y) = (0.32768, 0.32768)\n",
            "Iteration 6: (x, y) = (0.13107200000000002, 0.13107200000000002), f(x, y) = 0.03435973836800001, grad(x, y) = (0.26214400000000004, 0.26214400000000004)\n",
            "Iteration 7: (x, y) = (0.10485760000000002, 0.10485760000000002), f(x, y) = 0.02199023255552001, grad(x, y) = (0.20971520000000005, 0.20971520000000005)\n",
            "Iteration 8: (x, y) = (0.08388608000000002, 0.08388608000000002), f(x, y) = 0.014073748835532805, grad(x, y) = (0.16777216000000003, 0.16777216000000003)\n",
            "Iteration 9: (x, y) = (0.06710886400000002, 0.06710886400000002), f(x, y) = 0.009007199254740996, grad(x, y) = (0.13421772800000004, 0.13421772800000004)\n",
            "Iteration 10: (x, y) = (0.053687091200000016, 0.053687091200000016), f(x, y) = 0.005764607523034238, grad(x, y) = (0.10737418240000003, 0.10737418240000003)\n",
            "Iteration 11: (x, y) = (0.04294967296000001, 0.04294967296000001), f(x, y) = 0.0036893488147419122, grad(x, y) = (0.08589934592000002, 0.08589934592000002)\n",
            "Iteration 12: (x, y) = (0.034359738368000006, 0.034359738368000006), f(x, y) = 0.0023611832414348233, grad(x, y) = (0.06871947673600001, 0.06871947673600001)\n",
            "Iteration 13: (x, y) = (0.027487790694400004, 0.027487790694400004), f(x, y) = 0.001511157274518287, grad(x, y) = (0.05497558138880001, 0.05497558138880001)\n",
            "Iteration 14: (x, y) = (0.021990232555520003, 0.021990232555520003), f(x, y) = 0.0009671406556917036, grad(x, y) = (0.04398046511104001, 0.04398046511104001)\n",
            "Iteration 15: (x, y) = (0.017592186044416, 0.017592186044416), f(x, y) = 0.0006189700196426902, grad(x, y) = (0.035184372088832, 0.035184372088832)\n",
            "Iteration 16: (x, y) = (0.014073748835532801, 0.014073748835532801), f(x, y) = 0.00039614081257132176, grad(x, y) = (0.028147497671065603, 0.028147497671065603)\n",
            "Iteration 17: (x, y) = (0.01125899906842624, 0.01125899906842624), f(x, y) = 0.0002535301200456459, grad(x, y) = (0.02251799813685248, 0.02251799813685248)\n",
            "Iteration 18: (x, y) = (0.009007199254740993, 0.009007199254740993), f(x, y) = 0.00016225927682921338, grad(x, y) = (0.018014398509481985, 0.018014398509481985)\n",
            "Iteration 19: (x, y) = (0.007205759403792794, 0.007205759403792794), f(x, y) = 0.00010384593717069658, grad(x, y) = (0.014411518807585589, 0.014411518807585589)\n",
            "Iteration 20: (x, y) = (0.005764607523034235, 0.005764607523034235), f(x, y) = 6.64613997892458e-05, grad(x, y) = (0.01152921504606847, 0.01152921504606847)\n",
            "Iteration 21: (x, y) = (0.004611686018427388, 0.004611686018427388), f(x, y) = 4.2535295865117316e-05, grad(x, y) = (0.009223372036854777, 0.009223372036854777)\n",
            "Iteration 22: (x, y) = (0.0036893488147419105, 0.0036893488147419105), f(x, y) = 2.722258935367508e-05, grad(x, y) = (0.007378697629483821, 0.007378697629483821)\n",
            "Iteration 23: (x, y) = (0.0029514790517935286, 0.0029514790517935286), f(x, y) = 1.7422457186352053e-05, grad(x, y) = (0.005902958103587057, 0.005902958103587057)\n",
            "Iteration 24: (x, y) = (0.002361183241434823, 0.002361183241434823), f(x, y) = 1.1150372599265315e-05, grad(x, y) = (0.004722366482869646, 0.004722366482869646)\n",
            "Iteration 25: (x, y) = (0.0018889465931478584, 0.0018889465931478584), f(x, y) = 7.136238463529802e-06, grad(x, y) = (0.0037778931862957168, 0.0037778931862957168)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Newton Method with 2 Variables"
      ],
      "metadata": {
        "id": "lTS6SmYa7zxV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def function(x, y):\n",
        "    res = x**2 + y**2 - 3*x*y\n",
        "    return res\n",
        "\n",
        "def grad(x, y):\n",
        "    return np.array([2*x - 3*y, 2*y - 3*x])\n",
        "\n",
        "def hessian(x, y):\n",
        "    return np.array([\n",
        "        [2, -3],\n",
        "        [-3, 2]\n",
        "    ])\n",
        "\n",
        "# Define Newton Method\n",
        "def nm_optimization(x0, y0, n_iterations):\n",
        "    x, y = x0, y0\n",
        "    for i in range(n_iterations):\n",
        "        [x, y] = [x, y] - np.linalg.inv(hessian(x, y)).dot(grad(x, y))\n",
        "        print(f\"Iteration {i+1}: (x, y) = {(float(x), float(y))}, f(x, y) = {float(function(x, y))}, grad(x, y) = {grad(x, y)}, hessian(x, y): {hessian(x, y)}\")"
      ],
      "metadata": {
        "id": "1yUxqHFLPjgb"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Number of iterations\n",
        "n_iterations = 10\n",
        "\n",
        "# Initial guess\n",
        "x0, y0 = 0.5, 0.5\n",
        "\n",
        "# Run gradient descent\n",
        "nm_optimization(x0, y0, n_iterations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ujyrztv8_Jh",
        "outputId": "8ffa2f54-cd5a-47dc-b160-58c70d17c522"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1: (x, y) = (1.1102230246251565e-16, 0.0), f(x, y) = 1.232595164407831e-32, grad(x, y) = [ 2.22044605e-16 -3.33066907e-16], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 2: (x, y) = (3.697785493223493e-32, 0.0), f(x, y) = 1.367361755389411e-63, grad(x, y) = [ 7.39557099e-32 -1.10933565e-31], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 3: (x, y) = (1.642146637880645e-47, 2.7369110631344083e-48), f(x, y) = 1.4232296118264283e-94, grad(x, y) = [ 2.46321996e-47 -4.37905770e-47], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 4: (x, y) = (4.861730685829017e-63, 9.115745035929407e-64), f(x, y) = 1.117190412752074e-125, grad(x, y) = [ 6.98873786e-63 -1.27620431e-62], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 5: (x, y) = (1.0795210693868056e-78, 6.747006683667535e-79), f(x, y) = -5.644740299492344e-157, grad(x, y) = [ 1.34940134e-79 -1.88916187e-78], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 6: (x, y) = (2.3970182936024055e-94, 0.0), f(x, y) = 5.745696699864588e-188, grad(x, y) = [ 4.79403659e-94 -7.19105488e-94], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 7: (x, y) = (7.983674700015283e-110, 0.0), f(x, y) = 6.373906171566411e-219, grad(x, y) = [ 1.59673494e-109 -2.39510241e-109], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 8: (x, y) = (3.5454637892297225e-125, 5.909106315382871e-126), f(x, y) = 6.634332114834568e-250, grad(x, y) = [ 5.31819568e-125 -9.45457010e-125], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 9: (x, y) = (1.0496681418073576e-140, 1.9681277658887955e-141), f(x, y) = 5.207741724959478e-281, grad(x, y) = [ 1.50889795e-140 -2.75537887e-140], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n",
            "Iteration 10: (x, y) = (2.3307314785000646e-156, 1.4567071740625404e-156), f(x, y) = -2.631274780797e-312, grad(x, y) = [ 2.91341435e-157 -4.07878009e-156], hessian(x, y): [[ 2 -3]\n",
            " [-3  2]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J4pdCKlT-bha"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}